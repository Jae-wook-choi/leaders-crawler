import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st
import time

def scrape_news(start_page, end_page):
    news_data = []
    base_url = "https://finance.naver.com/news/mainnews.naver?page="

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }

    for page_num in range(start_page, end_page + 1):
        url = base_url + str(page_num)
        print(f"â–¶ í˜ì´ì§€ ìš”ì²­ ì¤‘: {url}")
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        news_items = soup.select(".newsList a[target='_blank']")

        for item in news_items:
            title = item.get_text(strip=True)
            link = item.get("href")
            if title and link:
                full_url = link if link.startswith("http") else f"https://finance.naver.com{link}"
                news_data.append({"ì œëª©": title, "URL": full_url})

        print(f"âœ… í˜ì´ì§€ {page_num} ì™„ë£Œ - {len(news_items)}ê±´ ìˆ˜ì§‘")
        time.sleep(1.5)

    return pd.DataFrame(news_data)

# Streamlit ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ“ˆ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
start_page = st.number_input("ì‹œì‘ í˜ì´ì§€", value=1)
end_page = st.number_input("ì¢…ë£Œ í˜ì´ì§€", value=3)

if st.button("í¬ë¡¤ë§ ì‹œì‘"):
    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        df = scrape_news(start_page, end_page)
        if not df.empty:
            st.success(f"{len(df)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            st.dataframe(df)
            df.to_excel("news.xlsx", index=False)
            with open("news.xlsx", "rb") as f:
                st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", f, file_name="news.xlsx")
        else:
            st.warning("ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")